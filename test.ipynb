{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23090128\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import sys\n",
    "sys.path.insert(0, 'src/')\n",
    "\n",
    "from src.model.model import VAE_Encoder\n",
    "from src.model.config import StableDiffusionConfig\n",
    "\n",
    "config = StableDiffusionConfig()\n",
    "enc = VAE_Encoder(config.img_channels, config.vae_features_dims, config.vae_num_groups, config.vae_num_heads, config.vae_dropout, config.vae_latent_dim)\n",
    "\n",
    "print(sum(p.numel() for p in enc.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 256, 256])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = torch.randn((1, config.img_channels, config.img_size, config.img_size))\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128, 256, 256])\n",
      "torch.Size([1, 4, 64, 64]) torch.Size([1, 4, 64, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0038,  0.1089, -0.0671,  ...,  0.2275, -0.4198,  0.1703],\n",
       "          [ 0.5301,  0.2587, -0.1111,  ..., -0.1029,  0.0550,  0.0756],\n",
       "          [-0.1987, -0.0749,  0.0266,  ...,  0.1186, -0.0274, -0.0080],\n",
       "          ...,\n",
       "          [ 0.1652, -0.0529,  0.0614,  ..., -0.0062,  0.1535,  0.0696],\n",
       "          [ 0.0269, -0.0185, -0.0709,  ...,  0.1139, -0.0032,  0.1059],\n",
       "          [ 0.0804, -0.4045, -0.0078,  ...,  0.0867, -0.1102, -0.0962]],\n",
       "\n",
       "         [[-0.3287,  0.2529, -0.0607,  ...,  0.2548, -0.3470, -0.0398],\n",
       "          [ 0.0159, -0.2010, -0.0110,  ..., -0.0085,  0.2650, -0.4200],\n",
       "          [-0.1653, -0.0298,  0.0943,  ..., -0.1496, -0.0508, -0.1952],\n",
       "          ...,\n",
       "          [-0.0898,  0.0291,  0.1399,  ..., -0.0397, -0.1191,  0.0355],\n",
       "          [-0.0578, -0.0025,  0.0878,  ..., -0.1553, -0.2446,  0.0215],\n",
       "          [ 0.0734, -0.1600, -0.0798,  ...,  0.0591,  0.2579,  0.0184]],\n",
       "\n",
       "         [[-0.3134,  0.0928,  0.1626,  ...,  0.1050,  0.2144, -0.0369],\n",
       "          [ 0.2391,  0.1702,  0.0910,  ...,  0.1113,  0.1325,  0.1707],\n",
       "          [ 0.0419,  0.2354,  0.3071,  ...,  0.0535,  0.1468,  0.4807],\n",
       "          ...,\n",
       "          [-0.1077,  0.0400, -0.1083,  ...,  0.0295, -0.0216,  0.3244],\n",
       "          [ 0.1260,  0.1091, -0.1045,  ...,  0.1352,  0.1945,  0.0111],\n",
       "          [ 0.1449,  0.1703,  0.3366,  ...,  0.2581,  0.1548, -0.2492]],\n",
       "\n",
       "         [[ 0.2382, -0.3894,  0.3873,  ..., -0.2309, -0.3500,  0.3113],\n",
       "          [-0.2627,  0.0280,  0.2182,  ...,  0.0845,  0.0029,  0.1697],\n",
       "          [-0.2169,  0.0808,  0.1434,  ...,  0.0250,  0.5403,  0.3048],\n",
       "          ...,\n",
       "          [ 0.2008,  0.1598,  0.1476,  ..., -0.0718,  0.0066, -0.3216],\n",
       "          [ 0.1775, -0.2825,  0.2536,  ..., -0.0940,  0.2633,  0.1279],\n",
       "          [ 0.2256,  0.0418, -0.1209,  ...,  0.2502,  0.2271,  0.2595]]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# noise = torch.randn((config.batch_size, config.vae_latent_dim // 2, config.img_size // 2**(len(config.vae_features_dims)-1), config.img_size // 2**(len(config.vae_features_dims)-1)))\n",
    "noise = torch.randn((1, 4, 64, 64))\n",
    "enc(img, noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Module [ModuleList] is missing the required \"forward\" function",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43menc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menc_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Илья\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Илья\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Илья\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:372\u001b[0m, in \u001b[0;36m_forward_unimplemented\u001b[1;34m(self, *input)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward_unimplemented\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    362\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Defines the computation performed at every call.\u001b[39;00m\n\u001b[0;32m    363\u001b[0m \n\u001b[0;32m    364\u001b[0m \u001b[38;5;124;03m    Should be overridden by all subclasses.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;124;03m        registered hooks while the latter silently ignores them.\u001b[39;00m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 372\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModule [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] is missing the required \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m function\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Module [ModuleList] is missing the required \"forward\" function"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
